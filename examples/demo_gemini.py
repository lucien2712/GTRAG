#!/usr/bin/env python3
"""
GTRAG API Usage Demo with Google Gemini Models

This example demonstrates how to use GTRAG with Google Gemini as both
the LLM and embedding model, showcasing the complete workflow:
1. System initialization with custom Gemini LLM/embedding functions
2. Document insertion with temporal metadata
3. Building temporal connections
4. Querying with entity/relation/chunk integration using Gemini models
using the new refactored gtrag package structure.

This demo shows the complete workflow:
1. System initialization with custom LLM/embedding functions
2. Document insertion with temporal metadata
3. Building temporal connections
4. Querying with entity/relation/chunk integration
"""
import os
import sys
from pathlib import Path

# Add project root to Python path to find gtrag module
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from gtrag import gtragSystem, QueryParams

# Optional dotenv loading
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("python-dotenv not available. Please set environment variables manually.")

# --- Custom Model Functions ---
# In real applications, you can put these functions in separate files
import google.generativeai as genai

# Ë®≠ÂÆö API ÈáëÈë∞ (ÈúÄË¶ÅÂÖàÂú®Áí∞Â¢ÉËÆäÊï∏Ë®≠ÂÆö GOOGLE_API_KEY)
import os
genai.configure(api_key="AIzaSyBrwJhQNHNdyqtA3K-7kM_zZmv8sJUPpDQ")

def gemini_llm(system_prompt: str, user_prompt: str) -> str:
    """Custom LLM function using Google Gemini 1.5 model, returns JSON string."""
    try:
        # Êää system prompt ÊîæÂà∞ system_instructionÔºåËÄå‰∏çÊòØ messages
        model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",            # ÂèØÊèõÊàê "gemini-1.5-pro"
            system_instruction=system_prompt
        )

        resp = model.generate_content(
            user_prompt,
            generation_config={
                "temperature": 0.1,
                # ËÆìÊ®°ÂûãËº∏Âá∫ JSONÔºàÂ≠ó‰∏≤ÂΩ¢ÊÖãÔºâÔºõ‰Ω†ÂéüÊú¨Áî®ÁöÑÊòØ OpenAI ÁöÑ response_format
                "response_mime_type": "application/json"
            }
        )
        # resp.text Â∑≤ÊòØÂ≠ó‰∏≤ÔºàÊ≠§ËôïÊúüÊúõÁÇ∫ JSON Â≠ó‰∏≤Ôºâ
        return resp.text or "{}"
    except Exception as e:
        print(f"Error calling Gemini: {e}")
        return "{}"

def gemini_embedding_func(text: str) -> list:
    """Custom embedding function using Google Gemini embedding model."""
    try:
        # Âª∫Ë≠∞‰ΩøÁî®ËºÉÊñ∞ÁöÑ text-embedding-004
        resp = genai.embed_content(
            model="models/text-embedding-004",
            content=text
        )
        # SDK ÊúÉÂõûÂÇ≥ {'embedding': [...]}
        emb = resp.get("embedding") if isinstance(resp, dict) else None
        return emb or []
    except Exception as e:
        print(f"Error generating embedding: {e}")
        return []
    
def main():
    """Main execution function"""
    print("=" * 60)
    print("gtrag API Usage Demo")
    print("=" * 60)
    
    # 1. Initialize System
    print("1. üöÄ Initializing gtrag System...")
    try:
        rag = gtragSystem(
            llm_func=gemini_llm,
            embedding_func=gemini_embedding_func
        )
        print("   ‚úÖ System initialization completed successfully")
    except Exception as e:
        print(f"   ‚ùå System initialization failed: {e}")
        return
    
    # 2. Index Documents with Temporal Metadata
    print("\n2. üìÑ Indexing documents with temporal metadata...")
    documents = [
        {
            "text": "Apple Inc. reported iPhone sales of 80 million units in Q4 2023, showing strong demand for the latest models.", 
            "doc_id": "apple_q4_2023", 
            "metadata": {"date": "2023Q4"}  # Quarter format
        },
        {
            "text": "Apple launched the iPhone 15 on September 15, 2023, featuring titanium design and enhanced camera capabilities.", 
            "doc_id": "apple_launch_sep_2023", 
            "metadata": {"date": "2023-09-15"}  # ISO date format
        },
        {
            "text": "Microsoft's cloud business revenue grew significantly by 30% in March 2024, driven by Azure services and enterprise adoption.", 
            "doc_id": "ms_march_2024", 
            "metadata": {"date": "2024-03"}  # Year-month format
        },
        {
            "text": "Google's annual developer conference in 2024 showcased major AI advances and new cloud computing capabilities.",
            "doc_id": "google_annual_2024",
            "metadata": {"date": "2024"}  # Year only format
        },
        {
            "text": "During Phase-Alpha of the Mars mission project, SpaceX successfully tested the new Raptor engine design for improved efficiency.",
            "doc_id": "spacex_phase_alpha",
            "metadata": {"date": "Phase-Alpha"}  # Custom label format
        }
    ]
    
    for i, doc in enumerate(documents, 1):
        try:
            rag.insert(doc["text"], doc["doc_id"], doc["metadata"])
            print(f"   ‚úÖ Indexed [{i}/{len(documents)}]: {doc['doc_id']}")
        except Exception as e:
            print(f"   ‚ùå Failed to index {doc['doc_id']}: {e}")
    
    # 3. Build Temporal Links
    print("\n3. üîó Building temporal connections...")
    try:
        rag.build_temporal_links()
        print("   ‚úÖ Temporal connections built successfully")
    except Exception as e:
        print(f"   ‚ùå Failed to build temporal connections: {e}")
        return
    
    # Get system statistics
    try:
        stats = rag.get_stats()
        print(f"\nüìä System Statistics:")
        print(f"   - Documents indexed: {stats.get('indexed_documents', 0)}")
        print(f"   - Graph nodes: {stats.get('num_nodes', 0)}")
        print(f"   - Graph edges: {stats.get('num_edges', 0)}")
        print(f"   - Stored chunks: {stats.get('stored_chunks', 0)}")
    except Exception as e:
        print(f"   ‚ùå Failed to get stats: {e}")
    
    # 4. Query Examples
    queries = [
        "What are the trends in Apple iPhone sales and product launches?",
    ]
    
    for i, question in enumerate(queries, 1):
        print(f"\n{3+i}. üîç Query {i}: {question}")
        print("-" * 80)
        
        # Define query parameters
        custom_query_params = QueryParams(
            top_k=8,
            similarity_threshold=0.2,
            max_hops=2,
            final_max_tokens=8000
        )
        
        try:
            result = rag.query(question, query_params=custom_query_params)
            
            # Display answer
            answer = result.get('answer', 'No answer generated')
            print(f"üìù Answer:\n{answer}")
            
            # Show retrieved context information
            entities = result.get('retrieved_entities', [])
            relations = result.get('retrieved_relations', [])
            chunks = result.get('retrieved_source_chunks', [])
            
            print(f"\nüìà Retrieved Context:")
            print(f"   - Entities: {len(entities)}")
            if entities:
                print("     Top entities:")
                for entity in entities[:3]:
                    name = entity.get('name', 'Unknown')
                    entity_type = entity.get('type', 'Unknown')
                    score = entity.get('score', 0)
                    print(f"       ‚Ä¢ {name} ({entity_type}) - Score: {score:.3f}")
            
            print(f"   - Relations: {len(relations)}")
            if relations:
                print("     Top relations:")
                for relation in relations[:3]:
                    source = relation.get('source', 'Unknown')
                    target = relation.get('target', 'Unknown')
                    rel_type = relation.get('type', 'Unknown')
                    score = relation.get('score', 0)
                    print(f"       ‚Ä¢ {source} ‚Üí {target} ({rel_type}) - Score: {score:.3f}")
            
            print(f"   - Source chunks: {len(chunks)}")
            
            # Token usage statistics
            token_stats = result.get('token_stats', {})
            print(f"   - Total tokens: {token_stats.get('total_tokens', 0)}")
            print(f"   - Entity tokens: {token_stats.get('entities_tokens', 0)}")
            print(f"   - Relation tokens: {token_stats.get('relations_tokens', 0)}")
            print(f"   - Chunk tokens: {token_stats.get('chunks_tokens', 0)}")
            
        except Exception as e:
            print(f"   ‚ùå Error processing query: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("üíæ Demonstrating Persistence & Reloading")
    print("=" * 60)
    
    # Save the knowledge graph to working directory
    working_dir = "./demo_gtrag_workspace/"
    print(f"\n8. üíæ Saving gtrag system to working directory '{working_dir}'...")
    try:
        save_result = rag.save_graph(working_dir)
        print(f"   ‚úÖ gtrag system saved successfully!")
        print(f"   üìÅ Working directory created: {save_result['working_dir']}")
        print(f"   üìÅ Files created:")
        print(f"      - graph.json (Knowledge graph with entities/relations)")
        print(f"      - chunks.json (Original text chunks for context)")
        if save_result.get('vectors_file'):
            print(f"      - vectors.faiss (Vector index for fast search)")
            print(f"      - vectors.metadata.npy (Vector metadata)")
        
        # Show directory contents
        from pathlib import Path
        work_path = Path(working_dir)
        if work_path.exists():
            files = list(work_path.glob("*"))
            print(f"   üìã Directory contents ({len(files)} files):")
            for file in sorted(files):
                size_kb = file.stat().st_size / 1024
                print(f"      - {file.name} ({size_kb:.1f} KB)")
                
    except Exception as e:
        print(f"   ‚ùå Failed to save system: {e}")
        import traceback
        traceback.print_exc()
    
    # Demonstrate loading in a new session
    print(f"\n9. üìÇ Loading gtrag system from working directory...")
    try:
        # Create a new RAG system instance (simulating new session)
        new_rag = gtragSystem(
            llm_func=gemini_llm,
            embedding_func=gemini_embedding_func
        )
        load_result = new_rag.load_graph(working_dir)
        print(f"   ‚úÖ gtrag system loaded successfully!")
        print(f"   üìÇ Loaded from: {load_result['working_dir']}")
        print(f"   üìä Components loaded:")
        print(f"      - Graph: {'‚úÖ' if load_result['loaded_graph'] else '‚ùå'}")
        print(f"      - Chunks: {'‚úÖ' if load_result['loaded_chunks'] else '‚ùå'}")
        print(f"      - Vectors: {'‚úÖ' if load_result['loaded_vectors'] else '‚ùå'}")
        
        # Get statistics from loaded system
        loaded_stats = new_rag.get_stats()
        print(f"   üìä Loaded system statistics:")
        print(f"      - Documents: {loaded_stats.get('indexed_documents', 0)}")
        print(f"      - Nodes: {loaded_stats.get('num_nodes', 0)}")  
        print(f"      - Edges: {loaded_stats.get('num_edges', 0)}")
        print(f"      - Chunks: {loaded_stats.get('stored_chunks', 0)}")
        
        # Test query on loaded system
        print(f"\n   üîç Testing query on loaded system...")
        test_result = new_rag.query("What did Apple achieve in 2024?", 
                                   query_params=QueryParams(top_k=5, similarity_threshold=0.3))
        print(f"      ‚úÖ Query successful! Answer length: {len(test_result.get('answer', ''))}")
        print(f"      üìä Retrieved: {len(test_result.get('retrieved_entities', []))} entities, "
              f"{len(test_result.get('retrieved_relations', []))} relations")
        
    except Exception as e:
        print(f"   ‚ùå Failed to load/test system: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("üéâ Demo Completed Successfully!")
    print("=" * 60)
    print(f"\nüí° You can now use the working directory:")
    print(f"   - To continue in another session: new_rag.load_graph('{working_dir}')")
    print(f"   - All components are neatly organized in one directory")
    print(f"   - Easy to backup, share, or deploy to production")
    print(f"   - Clean separation of concerns with standardized file names")
    
    # Show final system statistics
    try:
        final_stats = rag.get_stats()
        print(f"\nüìä Final System State:")
        for key, value in final_stats.items():
            print(f"   - {key}: {value}")
    except Exception as e:
        print(f"Failed to get final stats: {e}")


def demo_batch_processing():
    """Demonstrate batch processing capabilities"""
    print("\n" + "=" * 60)
    print("Batch Processing Demo")
    print("=" * 60)
    
    print("üìÅ Batch processing allows you to process multiple documents at once.")
    print("   Example usage:")
    print("""
    from gtrag.processing import BatchProcessor, BatchProcessingConfig
    
    # Configure batch processing
    config = BatchProcessingConfig(
        max_workers=4,
        batch_size=10,
        supported_formats=['.pdf', '.docx', '.txt', '.json']
    )
    
    # Initialize batch processor
    batch_processor = BatchProcessor(rag_system, config)
    
    # Process all documents in a directory
    results = batch_processor.process_directory("./documents/")
    
    # Process specific files
    file_list = ["doc1.pdf", "doc2.docx", "doc3.txt"]
    results = batch_processor.process_files(file_list)
    """)
    print("\n   üìù This would process multiple files concurrently with:")
    print("      ‚Ä¢ Automatic file format detection")
    print("      ‚Ä¢ Flexible time extraction from filenames/content (quarters, dates, years, custom labels)")
    print("      ‚Ä¢ Progress tracking and error handling")
    print("      ‚Ä¢ Parallel processing for better performance")


if __name__ == "__main__":
    try:
        main()
        demo_batch_processing()
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Demo interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demo failed with error: {e}")
        import traceback
        traceback.print_exc()